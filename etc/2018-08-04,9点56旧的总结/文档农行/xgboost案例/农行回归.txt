# -*- coding: utf-8 -*-
"""
Created on Fri Jul 20 10:58:02 2018


@author: 张博
"""

#读取csv最稳的方法:
#f = open(r'C:\Users\张博\Desktop\展示\old.csv')
#data = read_csv(f,header=None)





'''
画图模板:
from matplotlib import pyplot
data=[]
pyplot.plot(data,color='black')
pyplot.show()

'''



'''
获取当前时间:
import datetime
nowTime=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')#现在
nowTime=((nowTime)[:-3])
print(nowTime)
'''


'''
写文件的模板
with open(r'c:/234/wucha.txt','w') as f:
      wucha=str(wucha)
      f.write(wucha)
'''



'''
手动加断电的方法:raise 
'''








# -*- coding: utf-8 -*-
"""
Created on Fri Jul 20 10:58:02 2018


@author: 张博
"""

#读取csv最稳的方法:
#f = open(r'C:\Users\张博\Desktop\展示\old.csv')
#data = read_csv(f,header=None)





'''
画图模板:
from matplotlib import pyplot
data=[]
pyplot.plot(data,color='black')
pyplot.show()

'''



'''
获取当前时间:
import datetime
nowTime=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')#现在
nowTime=((nowTime)[:-3])
print(nowTime)
'''


'''
写文件的模板
with open(r'c:/234/wucha.txt','w') as f:
      wucha=str(wucha)
      f.write(wucha)
'''



'''
手动加断电的方法:raise 
'''


# -*- coding: utf-8 -*-
"""
Created on Fri Jul 20 10:58:02 2018


@author: 张博
"""









# -*- coding: utf-8 -*-
"""
Created on Tue Jul 17 10:54:38 2018

@author: 张博
"""

# -*- coding: utf-8 -*-
"""
Created on Mon Jul 16 17:18:57 2018

@author: 张博
"""

# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

#2018-07-23,22点54对学习率参数进行for循环来学习哪个最好RATE
for i in range((1)):
    
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0' #使用 GPU 0
    
    import tensorflow as tf
    from keras.backend.tensorflow_backend import set_session
    
    config = tf.ConfigProto()
    config.gpu_options.allocator_type = 'BFC' #A "Best-fit with coalescing" algorithm, simplified from a version of dlmalloc.
    config.gpu_options.per_process_gpu_memory_fraction = 1.
    config.gpu_options.allow_growth = True
    set_session(tf.Session(config=config))
    
    
    
    
    
    
    
    
    #老外的教程:非常详细,最后的多变量,多step模型应该是最终实际应用最好的模型了.也就是这个.py文件写的内容
    #https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/
    
    '''
    SUPER_PARAMETER:一般代码习惯把超参数写最开始位置,方便修改和查找
    '''
    EPOCH=100
    LOOK_BACK=24*2
    n_features = 3         #这个问题里面这个参数不用动,因为只有2个变量
    RATE=0.55
    shenjing=62
    n_hours = LOOK_BACK
    
    
    
    
    
    import pandas as pd
    
    from pandas import read_csv
    from datetime import datetime
    # load data
    def parse(x):
    	return datetime.strptime(x, '%Y %m %d %H')
    data = read_csv(r'E:\output_nonghang\out2new.csv')
    
    #应该把DD给删了,天数没用
    #切片和concat即可
    
    
    tmp1=data.iloc[:,2:3]
    tmp2=data.iloc[:,3]
    tmp3=data.iloc[:,1]

    data.to_csv('c:/234/out00000.csv')
    
#    for i in range(len(tmp3)):
#        if tmp3[i] in range(12,13):
#            tmp3[i]=1
#        if tmp3[i] in range(13,14):
#            tmp3[i]=2
#        else:
#            tmp3[i]=0


    #加一个预处理判断.判断数据奇异的点.
    #方法是:遍历一遍整个数据,如果这个点的数据比同时工作日或者周末的情况的mean的0.2还低
    #就说明这个点错了.用上面同比情况mean来替代.
    #2018-07-25,21点52跑出来百分之5.8错误率,说明这个修正的初始化过程非常重要!!不然就在
    #8左右徘徊.
    '''
    应该是更好的一种修改坏点的方法:
    比如7月23日3点的数据是错的.那么我们就用7月1日到7月23日2点的数据做训练,然后来预测7越23日3点的数据
    把这个7月23日3点预测到的数据当成真是数据来给7月23日3点.后面的坏点都同样处理.
    
    比如如果7月23日3点和4点数据都坏了.(也就是显然跟真实数据差很多,我的判断是比同期的数据0.4呗还低)
    那么我先预测3点的数据,然后把这个预测到的数据当真实值,4点的数据用上前面预测到的3点的值继续跑.来
    预测4点的值.这样就把3,4点的值都修正过来了.当然时间上会很慢,比下面使用的平均数替代法要多跑2次深度学习.
    '''

    for i in range(len(data)):
        hour=data.iloc[i]['HH']
        week=data.iloc[i]['week']
        tmp56=data.query('HH == '+str(hour) +' and '+ 'week=='+str(week)+' and '+'index!='+str(i))
        tmp_sum=tmp56['Sum'].mean()
        
        if data.iloc[i]['Sum']< tmp_sum *0.4:
            data.iloc[i]['Sum']=tmp_sum 
            print('修改了如下行,因为他是异常点')
            print(i)
            
            
    
    #修改完毕


    tmp1=data.iloc[:,2:3]
    tmp2=data.iloc[:,3]
    tmp3=data.iloc[:,1]















    









    
    
    data=pd.concat([tmp2,tmp3,tmp1],axis=1)
    print(data)
    
    
    
    
    
    data.to_csv('c:/234/out00000.csv')
    
    
    #因为下面的模板是把预测值放在了第一列.所以对data先做一个变换.
    
    
    
    
    
    
    
    
    
    
    
    
    
    #data.to_csv('pollution.csv')
    
    
    
    
    
    
    from pandas import read_csv
    from matplotlib import pyplot
    # load dataset
    dataset = data
    values = dataset.values
    
    
    
    ## specify columns to plot
    #groups = [0, 1, 2, 3, 5, 6, 7]
    #i = 1
    
    
    from pandas import read_csv
    from matplotlib import pyplot
    # load dataset
    #dataset = read_csv('pollution.csv', header=0, index_col=0)
    ##print(dataset.head())
    #values = dataset.values
    # specify columns to plot
    #groups = [0, 1, 2, 3, 5, 6, 7]
    #i = 1
    # plot each column
    #pyplot.figure()
    #图中每一行是一个列数据的展现.所以一共有7个小图,对应7个列指标的变化.
    #for group in groups:
    #	pyplot.subplot(len(groups), 1, i)
    #	pyplot.plot(values[:, group])
    #	pyplot.title(dataset.columns[group], y=0.5, loc='right')
    #	i += 1
    ##pyplot.show()
    
    
    
    from math import sqrt
    from numpy import concatenate
    from matplotlib import pyplot
    from pandas import read_csv
    from pandas import DataFrame
    from pandas import concat
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.preprocessing import LabelEncoder
    from sklearn.metrics import mean_squared_error
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import LSTM
    # load dataset
    
    
    # integer encode direction
    #把标签标准化而已.比如把1,23,5,7,7标准化之后就变成了0,1,2,3,3
    #print('values')
    #print(values[:5])
    #encoder = LabelEncoder()
    #values[:,4] = encoder.fit_transform(values[:,4])
    ## ensure all data is float
    #values = values.astype('float32')
    #print('values_after_endoding')
    #numpy 转pd
    import pandas as pd
    #pd.DataFrame(values).to_csv('values_after_endoding.csv')
    #从结果可以看出来encoder函数把这种catogorical的数据转化成了数值类型,
    #方便做回归.
    #print(values[:5])
    # normalize features,先正规化.
    
    
    
    
    #这里面系数多尝试(0,1) (-1,1) 或者用其他正则化方法.
    scaler = MinMaxScaler(feature_range=(-1, 1))
    scaled = scaler.fit_transform(values)
    print('正规化之后的数据')
    
    pd.DataFrame(scaled).to_csv('values_after_normalization.csv')
    
    # frame as supervised learning
    
    
    
    
    # convert series to supervised learning
    #n_in:之前的时间点读入多少,n_out:之后的时间点读入多少.
    #对于多变量,都是同时读入多少.为了方便,统一按嘴大的来.
    #print('测试shift函数')
    #
    #df = DataFrame(scaled)
    #print(df)      # 从测试看出来shift就是数据同时向下平移,或者向上平移.
    #print(df.shift(2))
    def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):
    	n_vars = 1 if type(data) is list else data.shape[1]
    	df = DataFrame(data)
    	cols, names = [],[]
    	# input sequence (t-n, ... t-1)
    	for i in range(n_in, 0, -1):
    		cols.append(df.shift(i))
    		names += [('var%d(时间:t-%s)' % (j+1, i)) for j in range(n_vars)]
    	# forecast sequence (t, t+1, ... t+n)
    	for i in range(0, n_out):
    		cols.append(df.shift(-i))
    		if i == 0:
    			names += [('var%d(时间:t)' % (j+1)) for j in range(n_vars)]
    		else:
    			names += [('var%d(时间:t+%d)' % (j+1, i)) for j in range(n_vars)]
    	# put it all together
    	agg = concat(cols, axis=1)
    	agg.columns = names
    	# drop rows with NaN values
    	if dropnan:
    		agg.dropna(inplace=True)
    	return agg
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    #series_to_supervised函数把多变量时间序列的列拍好.
    
    reframed = series_to_supervised(scaled, LOOK_BACK, 1)
    
    # drop columns we don't want to predict
    #我们只需要预测var1(t)所以把后面的拍都扔了.
    
    help111=series_to_supervised(values, LOOK_BACK, 1)
    
    
    
    print('处理的数据集')
    print(help111)
    
    # split into train and test sets
    values = reframed.values
    n_train_hours = int(len(scaled)*0.75)
    train = values[:n_train_hours, :]
    test = values[n_train_hours:, :]
    # split into input and outputs
    n_obs = n_hours * n_features
    train_X, train_y = train[:, :n_obs], train[:, -n_features]
    test_X, test_y = test[:, :n_obs], test[:, -n_features]
    #print(train_X.shape, len(train_X), train_y.shape)
    #print(test_X.shape, len(test_X), test_y.shape)
    #print(train_X)
    #print(9999999999999999)
    #print(test_X)
    
    
    
    
    
    '''
    所以最后我们得到4个数据
    train_X
    train_Y
    test_X
    test_Y
    '''
    
    
    
    
    
    #下面我开始改成xgboost来跑
#    print(train_X.shape)
#    print(train_y.shape)
#    print(test_X.shape)
#    print(test_y.shape)
    
    
    
    '''
    Learning Task Parameters

Specify the learning task and the corresponding learning objective. The objective options are below:

objective [default=reg:linear]
reg:linear: linear regression
reg:logistic: logistic regression
binary:logistic: logistic regression for binary classification, output probability
binary:logitraw: logistic regression for binary classification, output score before logistic transformation
gpu:reg:linear, gpu:reg:logistic, gpu:binary:logistic, gpu:binary:logitraw: versions of the corresponding objective functions evaluated on the GPU; note that like the GPU histogram algorithm, they can only be used when the entire training session uses the same dataset
count:poisson Cpoisson regression for count data, output mean of poisson distribution
max_delta_step is set to 0.7 by default in poisson regression (used to safeguard optimization)
survival:cox: Cox regression for right censored survival time data (negative values are considered right censored). Note that predictions are returned on the hazard ratio scale (i.e., as HR = exp(marginal_prediction) in the proportional hazard function h(t) = h0(t) * HR).
multi:softmax: set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class(number of classes)
multi:softprob: same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class.
rank:pairwise: set XGBoost to do ranking task by minimizing the pairwise loss
reg:gamma: gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed.
reg:tweedie: Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed.
base_score [default=0.5]
The initial prediction score of all instances, global bias
For sufficient number of iterations, changing this value will not have too much effect.
eval_metric [default according to objective]
Evaluation metrics for validation data, a default metric will be assigned according to objective (rmse for regression, and error for classification, mean average precision for ranking)
User can add multiple evaluation metrics. Python users: remember to pass the metrics in as list of parameters pairs instead of map, so that latter eval_metric won’t override previous one
The choices are listed below:
rmse: root mean square error
mae: mean absolute error
logloss: negative log-likelihood
error: Binary classification error rate. It is calculated as #(wrong cases)/#(all cases). For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.
error@t: a different than 0.5 binary classification threshold value could be specified by providing a numerical value through ‘t’.
merror: Multiclass classification error rate. It is calculated as #(wrong cases)/#(all cases).
mlogloss: Multiclass logloss.
auc: Area under the curve
ndcg: Normalized Discounted Cumulative Gain
map: Mean average precision
ndcg@n, map@n: ‘n’ can be assigned as an integer to cut off the top positions in the lists for evaluation.
ndcg-, map-, ndcg@n-, map@n-: In XGBoost, NDCG and MAP will evaluate the score of a list without any positive samples as 1. By adding “-” in the evaluation metric XGBoost will evaluate these score as 0 to be consistent under some conditions.
poisson-nloglik: negative log-likelihood for Poisson regression
gamma-nloglik: negative log-likelihood for gamma regression
cox-nloglik: negative partial log-likelihood for Cox proportional hazards regression
gamma-deviance: residual deviance for gamma regression
tweedie-nloglik: negative log-likelihood for Tweedie regression (at a specified value of the tweedie_variance_power parameter)
seed [default=0]
Random number seed.
    '''
    
    #回归

        
    model = xgb.XGBRegressor(max_depth=15, learning_rate=0.1,
                             n_estimators=1600, silent=True, objective='reg:linear')
    model.fit(train_X, train_y)
    
    # 对测试集进行预测
    ans = model.predict(test_X)
    yhat=ans
    # 显示重要特征
    import xgboost

    xgboost.plot_importance(model,height=30)
    
    plt.show()    
 
#    xgboost.plot_tree(model)
#    
#    plt.show()   
    
    
    
    
    
    test_X = test_X.reshape((test_X.shape[0], n_hours*n_features))
    # invert scaling for forecast
    
    
    
    import numpy as np
    yhat=yhat.reshape(len(yhat),1)
    print(yhat.shape)
    print(test_X[:, -(n_features-1):].shape)
    
    #因为之前的scale是对初始数据做scale的,inverse回去还需要把矩阵的型拼回去.
    inv_yhat = np.concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)
    inv_yhat = scaler.inverse_transform(inv_yhat)
    inv_yhat = inv_yhat[:,0]#inverse完再把数据扣出来.多变量这个地方需要的操作要多点
    # invert scaling for actual
    
    
    
    test_y = test_y.reshape((len(test_y), 1))
    inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)
    inv_y = scaler.inverse_transform(inv_y)
    inv_y = inv_y[:,0]
    
    
    
    
    
    
    
    
    
    with open(r'c:/234/inv_y.txt','w') as f:
          inv_y1=str(inv_y)
          f.write(inv_y1)
    with open(r'c:/234/inv_yhat.txt','w') as f:
          inv_yhat1=str(inv_yhat)
          f.write(inv_yhat1)
    
    
    
    # calculate RMSE
    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))
#    print('RATE:')
#    print(RATE)
    print('输出abs差百分比指标:')
    #这个污染指数还有0的.干扰非常大
    #print(inv_y.shape)
    #print(inv_yhat.shape)
    wucha=abs(inv_y-inv_yhat)/(inv_y)
    #print(wucha)
    '''
    下面把得到的abs百分比误差写到 文件里面
    '''

    #with open(r'c:/234/wucha.txt','w') as f:
    #      print(type(wucha))
    #      wucha2=list(wucha)
    #      wucha2=str(wucha2)
    #      f.write(wucha2)
    
    with open(r'c:/234/sumary.txt','a') as f:
          rate=str(RATE)
          f.write(rate+'，')
          shenjing=str(shenjing)
          f.write(shenjing)
          f.write(',')
          wucha2=wucha.mean()
          wucha2=str(wucha2)
          f.write(wucha2)
          f.write('.')
          f.write('\n')
    
    
    wucha=wucha.mean()
    print(wucha)
    
    
    
    inv_y=inv_y
    inv_yhat=inv_yhat
    
    #print('Test RMSE: %.3f' % rmse)
    import numpy as np
    
    from matplotlib import pyplot
    pyplot.rcParams['figure.figsize'] = (20, 3) # 设置figure_size尺寸
    
    pyplot.rcParams['image.cmap'] = 'gray' # 
    pyplot.plot(inv_y,color='black',linewidth = 0.7)
    pyplot.plot(inv_yhat     ,color='red',linewidth = 0.7)
    
    pyplot.show()
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    '''
    获取当前时间:
    import datetime
    nowTime=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')#现在
    nowTime=((nowTime)[:-3])
    print(nowTime)
    '''
    
    
    '''
    写文件的模板
    with open(r'c:/234/wucha.txt','w') as f:
          wucha=str(wucha)
          f.write(wucha)
    '''
    
    
    
    
    
    
    '''
    手动加断电的方法:raise NameError #这种加断点方法靠谱
    '''
    
    '''
    画图模板:
    import numpy as np
    
    from matplotlib import pyplot
    pyplot.rcParams['figure.figsize'] = (20, 3) # 设置figure_size尺寸
    
    pyplot.rcParams['image.cmap'] = 'gray' # 
    pyplot.plot(inv_y,color='black',linewidth = 0.7)
    
    
    pyplot.show()
    
    
    '''
    
    #读取csv最稳的方法:
    #f = open(r'C:\Users\张博\Desktop\展示\old.csv')
    #data = read_csv(f,header=None)
    


























































